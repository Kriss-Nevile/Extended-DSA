{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "427d7159",
   "metadata": {},
   "source": [
    "# Duplicate / Near-Duplicate Search with Embeddings, SimHash & MinHash\n",
    "\n",
    "Notebook này thực hiện pipeline:\n",
    "\n",
    "1. Load embedding từ file NPZ (ví dụ: `bge-base-en-v1.5_pair_embeddings.npz`).\n",
    "2. Lấy mẫu một số cặp văn bản (mặc định 1000 cặp).\n",
    "3. Ghép thành một tập document embedding chung (~2M documents cho M cặp).\n",
    "4. Xây chỉ mục tìm kiếm tương đồng với các lựa chọn:\n",
    "   - Exact cosine (không dùng hash).\n",
    "   - SimHash + LSH trên embedding.\n",
    "   - MinHash (top-k chiều embedding) + LSH (banding).\n",
    "5. Đánh giá khả năng tìm lại văn bản trùng/near-duplicate bằng nhãn `is_duplicate` thông qua Recall@K.\n",
    "\n",
    "Bạn cần sửa đường dẫn `EMBEDDING_FILE` cho phù hợp với vị trí file embedding trong Google Drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acba505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kết nối Google Drive (bỏ qua nếu không chạy trên Colab)\n",
    "try:\n",
    "    from google.colab import drive  # type: ignore\n",
    "    drive.mount('/content/drive')\n",
    "    print('Đã mount Google Drive.')\n",
    "except ImportError:\n",
    "    print('Không phải môi trường Colab, bỏ qua bước mount Drive.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef9f164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# =============================\n",
    "# Cấu hình thí nghiệm\n",
    "# =============================\n",
    "\n",
    "# TODO: Sửa đường dẫn này cho đúng với file embedding của bạn\n",
    "# Ví dụ trên Colab: '/content/drive/MyDrive/your_folder/bge-base-en-v1.5_pair_embeddings.npz'\n",
    "EMBEDDING_FILE = '/content/drive/MyDrive/path/to/bge-base-en-v1.5_pair_embeddings.npz'\n",
    "\n",
    "@dataclass\n",
    "class ExperimentConfig:\n",
    "    embedding_file: str\n",
    "    sample_pairs: int      # số cặp (row) sẽ lấy mẫu\n",
    "    hash_method: str       # 'none' | 'simhash' | 'minhash'\n",
    "\n",
    "    # Cho SimHash\n",
    "    n_bits: int = 64       # số bit simhash\n",
    "\n",
    "    # Cho MinHash\n",
    "    n_hashes: int = 128    # số hash function\n",
    "    topk_dims: int = 20    # số chiều top-k của embedding dùng làm \"set\"\n",
    "    n_bands: int = 16      # số band cho LSH (n_hashes phải chia hết cho n_bands)\n",
    "\n",
    "    top_k: int = 10        # số hàng xóm cần tìm khi search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ac0dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Hàm load embedding & sampling\n",
    "# =============================\n",
    "\n",
    "def load_embeddings_npz(path: str):\n",
    "    \"\"\"Load NPZ có 3 mảng: text1_embeddings, text2_embeddings, is_duplicate.\"\"\"\n",
    "    data = np.load(path)\n",
    "    E1 = data['text1_embeddings']  # (N, D)\n",
    "    E2 = data['text2_embeddings']  # (N, D)\n",
    "    labels = data['is_duplicate']  # (N,)\n",
    "    return E1, E2, labels\n",
    "\n",
    "\n",
    "def sample_pairs(E1: np.ndarray, E2: np.ndarray, labels: np.ndarray, n_pairs: int, seed: int = 42):\n",
    "    \"\"\"Lấy ngẫu nhiên n_pairs cặp từ toàn bộ embedding.\"\"\"\n",
    "    N = E1.shape[0]\n",
    "    n_pairs = min(n_pairs, N)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = rng.choice(N, size=n_pairs, replace=False)\n",
    "    return E1[idx], E2[idx], labels[idx]\n",
    "\n",
    "\n",
    "def build_corpus(E1_sampled: np.ndarray, E2_sampled: np.ndarray, labels_sampled: np.ndarray):\n",
    "    \"\"\"Ghép thành tập docs chung và lưu mapping cho từng cặp.\n",
    "\n",
    "    E1_sampled: (M, D)\n",
    "    E2_sampled: (M, D)\n",
    "    labels_sampled: (M,)\n",
    "    Trả về:\n",
    "        docs: (2M, D)\n",
    "        pair_info: list dict với keys: pair_index, doc1_id, doc2_id, label\n",
    "    \"\"\"\n",
    "    M, D = E1_sampled.shape\n",
    "    docs = np.vstack([E1_sampled, E2_sampled])  # (2M, D)\n",
    "\n",
    "    pair_info = []\n",
    "    for i in range(M):\n",
    "        pair_info.append({\n",
    "            'pair_index': int(i),\n",
    "            'doc1_id': int(i),\n",
    "            'doc2_id': int(i + M),\n",
    "            'label': int(labels_sampled[i]),\n",
    "        })\n",
    "    return docs, pair_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49350b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Hashing: SimHash & MinHash\n",
    "# =============================\n",
    "\n",
    "class HashEncoder:\n",
    "    def fit(self, docs: np.ndarray):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def encode(self, docs: np.ndarray) -> np.ndarray:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class SimHashEncoder(HashEncoder):\n",
    "    \"\"\"SimHash trên embedding: dùng random hyperplanes để tạo mã nhị phân.\"\"\"\n",
    "    def __init__(self, n_bits: int = 64, seed: int = 42):\n",
    "        self.n_bits = n_bits\n",
    "        self.seed = seed\n",
    "        self.random_planes = None  # (n_bits, D)\n",
    "\n",
    "    def fit(self, docs: np.ndarray):\n",
    "        N, D = docs.shape\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        self.random_planes = rng.standard_normal(size=(self.n_bits, D))\n",
    "\n",
    "    def encode(self, docs: np.ndarray) -> np.ndarray:\n",
    "        projections = docs @ self.random_planes.T  # (N, n_bits)\n",
    "        bits = (projections >= 0).astype(np.uint8)\n",
    "        return bits\n",
    "\n",
    "\n",
    "class MinHashEncoder(HashEncoder):\n",
    "    \"\"\"MinHash trên embedding.\n",
    "\n",
    "    Ý tưởng:\n",
    "    - Mỗi vector -> set các index top-k theo |giá trị|.\n",
    "    - Dùng num_hashes hàm hash dạng (a_i * x + b_i) mod P.\n",
    "    - Signature[j] = min hash_j(s) trên s thuộc set.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_hashes: int = 128, topk_dims: int = 20, seed: int = 42):\n",
    "        self.num_hashes = num_hashes\n",
    "        self.topk_dims = topk_dims\n",
    "        self.seed = seed\n",
    "        self.prime = None\n",
    "        self.a = None\n",
    "        self.b = None\n",
    "        self.dim = None\n",
    "\n",
    "    @staticmethod\n",
    "    def _next_prime(n: int) -> int:\n",
    "        def is_prime(x: int) -> bool:\n",
    "            if x < 2:\n",
    "                return False\n",
    "            if x % 2 == 0:\n",
    "                return x == 2\n",
    "            r = int(math.isqrt(x))\n",
    "            for i in range(3, r + 1, 2):\n",
    "                if x % i == 0:\n",
    "                    return False\n",
    "            return True\n",
    "\n",
    "        while not is_prime(n):\n",
    "            n += 1\n",
    "        return n\n",
    "\n",
    "    def _embedding_to_set_topk(self, vec: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"vec: (D,) -> mảng index top-k theo |vec|.\"\"\"\n",
    "        D = vec.shape[0]\n",
    "        k = min(self.topk_dims, D)\n",
    "        if k <= 0:\n",
    "            return np.array([], dtype=np.int64)\n",
    "        # top-k theo |giá trị| (không cần sắp xếp toàn bộ)\n",
    "        idx = np.argpartition(-np.abs(vec), k - 1)[:k]\n",
    "        return idx.astype(np.int64)\n",
    "\n",
    "    def fit(self, docs: np.ndarray):\n",
    "        N, D = docs.shape\n",
    "        self.dim = D\n",
    "        # P là số nguyên tố > D\n",
    "        self.prime = self._next_prime(D + 1)\n",
    "\n",
    "        rng = np.random.default_rng(self.seed)\n",
    "        self.a = rng.integers(1, self.prime, size=self.num_hashes, endpoint=False, dtype=np.int64)\n",
    "        self.b = rng.integers(0, self.prime, size=self.num_hashes, endpoint=False, dtype=np.int64)\n",
    "\n",
    "    def encode(self, docs: np.ndarray) -> np.ndarray:\n",
    "        N, D = docs.shape\n",
    "        assert self.dim == D, 'Docs dimension khác với lúc fit MinHashEncoder'\n",
    "\n",
    "        sigs = np.full((N, self.num_hashes), fill_value=self.prime + 1, dtype=np.int64)\n",
    "\n",
    "        for i in range(N):\n",
    "            idx = self._embedding_to_set_topk(docs[i])\n",
    "            if idx.size == 0:\n",
    "                continue\n",
    "            # hashes: (num_hashes, |set|)\n",
    "            hashes = (self.a[:, None] * idx[None, :] + self.b[:, None]) % self.prime\n",
    "            sigs[i, :] = hashes.min(axis=1)\n",
    "\n",
    "        return sigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d9adaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Index cho similarity search\n",
    "# =============================\n",
    "\n",
    "class BaseIndex:\n",
    "    def build(self, docs: np.ndarray, signatures: np.ndarray = None):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def query(self, query_vec: np.ndarray, top_k: int = 10, encoder: HashEncoder = None):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class ExactCosineIndex(BaseIndex):\n",
    "    \"\"\"Search exact theo cosine similarity (baseline).\"\"\"\n",
    "    def build(self, docs: np.ndarray, signatures: np.ndarray = None):\n",
    "        self.docs = docs\n",
    "        norms = np.linalg.norm(docs, axis=1, keepdims=True) + 1e-9\n",
    "        self.normalized = docs / norms\n",
    "\n",
    "    def query(self, query_vec: np.ndarray, top_k: int = 10, encoder: HashEncoder = None):\n",
    "        q = query_vec / (np.linalg.norm(query_vec) + 1e-9)\n",
    "        scores = self.normalized @ q  # (N,)\n",
    "        top_idx = np.argpartition(-scores, top_k)[:top_k]\n",
    "        top_idx = top_idx[np.argsort(-scores[top_idx])]\n",
    "        return [(int(i), float(scores[i])) for i in top_idx]\n",
    "\n",
    "\n",
    "class SimHashLSHIndex(BaseIndex):\n",
    "    \"\"\"LSH cho SimHash: bucket theo mã nhị phân.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.docs = None\n",
    "        self.bits = None\n",
    "        self.hash_table = {}\n",
    "\n",
    "    @staticmethod\n",
    "    def bits_to_key(bits_row: np.ndarray) -> str:\n",
    "        return ''.join('1' if b else '0' for b in bits_row)\n",
    "\n",
    "    def build(self, docs: np.ndarray, signatures: np.ndarray):\n",
    "        self.docs = docs\n",
    "        self.bits = signatures\n",
    "        self.hash_table = {}\n",
    "        N = docs.shape[0]\n",
    "        for doc_id in range(N):\n",
    "            key = self.bits_to_key(signatures[doc_id])\n",
    "            self.hash_table.setdefault(key, []).append(doc_id)\n",
    "\n",
    "    def query(self, query_vec: np.ndarray, top_k: int = 10, encoder: HashEncoder = None):\n",
    "        assert encoder is not None, 'Cần SimHashEncoder để query'\n",
    "        q_bits = encoder.encode(query_vec[None, :])[0]\n",
    "        key = self.bits_to_key(q_bits)\n",
    "        candidates = self.hash_table.get(key, [])\n",
    "        if not candidates:\n",
    "            return []\n",
    "\n",
    "        docs = self.docs[candidates]\n",
    "        norms = np.linalg.norm(docs, axis=1, keepdims=True) + 1e-9\n",
    "        normalized_docs = docs / norms\n",
    "        q = query_vec / (np.linalg.norm(query_vec) + 1e-9)\n",
    "\n",
    "        scores = normalized_docs @ q  # (len(candidates),)\n",
    "        idx = np.argsort(-scores)[:top_k]\n",
    "        result_ids = [candidates[i] for i in idx]\n",
    "        return [(int(doc_id), float(scores[i])) for doc_id, i in zip(result_ids, idx)]\n",
    "\n",
    "\n",
    "class MinHashLSHIndex(BaseIndex):\n",
    "    \"\"\"LSH cho MinHash bằng banding.\"\"\"\n",
    "    def __init__(self, num_bands: int = 16):\n",
    "        self.num_bands = num_bands\n",
    "        self.hash_tables = []\n",
    "        self.docs = None\n",
    "        self.signatures = None\n",
    "        self.rows_per_band = None\n",
    "\n",
    "    def build(self, docs: np.ndarray, signatures: np.ndarray):\n",
    "        self.docs = docs\n",
    "        self.signatures = signatures\n",
    "        N, num_hashes = signatures.shape\n",
    "        assert num_hashes % self.num_bands == 0, 'num_hashes phải chia hết cho num_bands'\n",
    "\n",
    "        self.rows_per_band = num_hashes // self.num_bands\n",
    "        self.hash_tables = [dict() for _ in range(self.num_bands)]\n",
    "\n",
    "        for doc_id in range(N):\n",
    "            sig = signatures[doc_id]\n",
    "            for b in range(self.num_bands):\n",
    "                start = b * self.rows_per_band\n",
    "                end = start + self.rows_per_band\n",
    "                band_tuple = tuple(sig[start:end].tolist())\n",
    "                table = self.hash_tables[b]\n",
    "                table.setdefault(band_tuple, []).append(doc_id)\n",
    "\n",
    "    def query(self, query_vec: np.ndarray, top_k: int = 10, encoder: HashEncoder = None):\n",
    "        assert encoder is not None, 'Cần MinHashEncoder để query'\n",
    "        q_sig = encoder.encode(query_vec[None, :])[0]\n",
    "\n",
    "        candidates = set()\n",
    "        for b in range(self.num_bands):\n",
    "            start = b * self.rows_per_band\n",
    "            end = start + self.rows_per_band\n",
    "            band_tuple = tuple(q_sig[start:end].tolist())\n",
    "            table = self.hash_tables[b]\n",
    "            if band_tuple in table:\n",
    "                for doc_id in table[band_tuple]:\n",
    "                    candidates.add(doc_id)\n",
    "\n",
    "        candidates = list(candidates)\n",
    "        if not candidates:\n",
    "            return []\n",
    "\n",
    "        docs = self.docs[candidates]\n",
    "        norms = np.linalg.norm(docs, axis=1, keepdims=True) + 1e-9\n",
    "        normalized_docs = docs / norms\n",
    "        q = query_vec / (np.linalg.norm(query_vec) + 1e-9)\n",
    "\n",
    "        scores = normalized_docs @ q\n",
    "        idx = np.argsort(-scores)[:top_k]\n",
    "        result_ids = [candidates[i] for i in idx]\n",
    "        return [(int(doc_id), float(scores[i])) for doc_id, i in zip(result_ids, idx)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b909cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Evaluation với is_duplicate\n",
    "# =============================\n",
    "\n",
    "def evaluate_recall_at_k(index: BaseIndex,\n",
    "                         docs: np.ndarray,\n",
    "                         pair_info: list,\n",
    "                         k: int,\n",
    "                         encoder: HashEncoder = None):\n",
    "    \"\"\"Tính Recall@K: doc2_id của các cặp label=1 có xuất hiện trong top-K khi query doc1_id không.\"\"\"\n",
    "    hits = 0\n",
    "    total = 0\n",
    "\n",
    "    for p in pair_info:\n",
    "        if p['label'] != 1:\n",
    "            continue\n",
    "        total += 1\n",
    "        doc1_id = p['doc1_id']\n",
    "        doc2_id = p['doc2_id']\n",
    "\n",
    "        query_vec = docs[doc1_id]\n",
    "        neighbors = index.query(query_vec, top_k=k, encoder=encoder)\n",
    "        retrieved_ids = [doc_id for doc_id, _ in neighbors]\n",
    "        if doc2_id in retrieved_ids:\n",
    "            hits += 1\n",
    "\n",
    "    recall_at_k = hits / total if total > 0 else 0.0\n",
    "    return recall_at_k, hits, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4292b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# Hàm chạy toàn bộ thí nghiệm\n",
    "# =============================\n",
    "\n",
    "def run_experiment(cfg: ExperimentConfig):\n",
    "    print('Embedding file:', cfg.embedding_file)\n",
    "    print('Sample pairs:', cfg.sample_pairs)\n",
    "    print('Hash method:', cfg.hash_method)\n",
    "    print('-----------------------------')\n",
    "\n",
    "    # 1) Load & sample\n",
    "    E1, E2, labels = load_embeddings_npz(cfg.embedding_file)\n",
    "    print('Tổng số cặp ban đầu:', E1.shape[0])\n",
    "    E1_s, E2_s, labels_s = sample_pairs(E1, E2, labels, cfg.sample_pairs)\n",
    "    docs, pair_info = build_corpus(E1_s, E2_s, labels_s)\n",
    "    print('Số docs trong corpus:', docs.shape[0])\n",
    "    print('Số cặp sau khi sample:', len(pair_info))\n",
    "\n",
    "    # 2) Build index\n",
    "    if cfg.hash_method == 'none':\n",
    "        encoder = None\n",
    "        index = ExactCosineIndex()\n",
    "        index.build(docs)\n",
    "\n",
    "    elif cfg.hash_method == 'simhash':\n",
    "        encoder = SimHashEncoder(n_bits=cfg.n_bits)\n",
    "        encoder.fit(docs)\n",
    "        signatures = encoder.encode(docs)\n",
    "        index = SimHashLSHIndex()\n",
    "        index.build(docs, signatures)\n",
    "\n",
    "    elif cfg.hash_method == 'minhash':\n",
    "        assert cfg.n_hashes % cfg.n_bands == 0, 'n_hashes phải chia hết cho n_bands'\n",
    "        encoder = MinHashEncoder(num_hashes=cfg.n_hashes, topk_dims=cfg.topk_dims)\n",
    "        encoder.fit(docs)\n",
    "        signatures = encoder.encode(docs)\n",
    "        index = MinHashLSHIndex(num_bands=cfg.n_bands)\n",
    "        index.build(docs, signatures)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f'Unknown hash_method: {cfg.hash_method}')\n",
    "\n",
    "    # 3) Evaluate Recall@K\n",
    "    for k in [1, 5, 10]:\n",
    "        recall_k, hits, total = evaluate_recall_at_k(index, docs, pair_info, k, encoder=encoder)\n",
    "        print(f'Recall@{k}: {recall_k:.4f}  (hits={hits} / total positives={total})')\n",
    "\n",
    "\n",
    "# Ví dụ chạy thử: dùng MinHash trên 1000 cặp\n",
    "cfg = ExperimentConfig(\n",
    "    embedding_file=EMBEDDING_FILE,\n",
    "    sample_pairs=1000,\n",
    "    hash_method='minhash',   # 'none' | 'simhash' | 'minhash'\n",
    "    n_bits=64,\n",
    "    n_hashes=128,\n",
    "    topk_dims=20,\n",
    "    n_bands=16,\n",
    "    top_k=10,\n",
    ")\n",
    "\n",
    "print('Cấu hình thí nghiệm:', cfg)\n",
    "run_experiment(cfg)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
