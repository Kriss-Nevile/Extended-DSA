# -*- coding: utf-8 -*-
"""minhash_simhash_V4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13_L7UPkDbks2WantNC_UZhv8jREB6RTa

# Duplicate / Near-Duplicate Search with Embeddings, SimHash & MinHash

Notebook này thực hiện pipeline:

1. Load embedding từ file NPZ (ví dụ: `bge-base-en-v1.5_pair_embeddings.npz`).
2. Lấy mẫu một số cặp văn bản (mặc định 1000 cặp).
3. Ghép thành một tập document embedding chung (~2M documents cho M cặp).
4. Xây chỉ mục tìm kiếm tương đồng với các lựa chọn:
   - Exact cosine (không dùng hash).
   - SimHash + LSH trên embedding.
5. Đánh giá khả năng tìm lại văn bản trùng/near-duplicate bằng nhãn `is_duplicate` thông qua Recall@K.

Bạn cần sửa đường dẫn `EMBEDDING_FILE` cho phù hợp với vị trí file embedding trong Google Drive.
"""

# Kết nối Google Drive (bỏ qua nếu không chạy trên Colab)
try:
    from google.colab import drive  # type: ignore
    drive.mount('/content/drive')
    print('Đã mount Google Drive.')
except ImportError:
    print('Không phải môi trường Colab, bỏ qua bước mount Drive.')

import os
import math
import numpy as np
from dataclasses import dataclass

# =============================
# Cấu hình thí nghiệm
# =============================

# TODO: Sửa đường dẫn này cho đúng với file embedding của bạn
# Ví dụ trên Colab: '/content/drive/MyDrive/your_folder/bge-base-en-v1.5_pair_embeddings.npz'
EMBEDDING_FILE = '/content/drive/MyDrive/Colab Notebooks/Extend_dsa/Data/embeddings/e5-base-v2_pair_embeddings.npz'

@dataclass
class ExperimentConfig:
    embedding_file: str
    sample_pairs: int      # số cặp (row) sẽ lấy mẫu
    hash_method: str       # 'none' | 'simhash' | 'minhash'

    # Cho SimHash
    n_bits: int = 64       # số bit simhash

    # Cho MinHash
    n_hashes: int = 128    # số hash function
    topk_dims: int = 20    # số chiều top-k của embedding dùng làm "set"
    n_bands: int = 16      # số band cho LSH (n_hashes phải chia hết cho n_bands)

    top_k: int = 10        # số hàng xóm cần tìm khi search

# =============================
# Hàm load embedding & sampling
# =============================

def load_embeddings_npz(path: str):
    """Load NPZ có 3 mảng: text1_embeddings, text2_embeddings, is_duplicate."""
    data = np.load(path)
    E1 = data['text1_embeddings']  # (N, D)
    E2 = data['text2_embeddings']  # (N, D)
    labels = data['is_duplicate']  # (N,)
    return E1, E2, labels


def sample_pairs(E1: np.ndarray, E2: np.ndarray, labels: np.ndarray, n_pairs: int, seed: int = 42):
    """Lấy ngẫu nhiên n_pairs cặp từ toàn bộ embedding."""
    N = E1.shape[0]
    n_pairs = min(n_pairs, N)
    rng = np.random.default_rng(seed)
    idx = rng.choice(N, size=n_pairs, replace=False)
    return E1[idx], E2[idx], labels[idx]


def build_corpus(E1_sampled: np.ndarray, E2_sampled: np.ndarray, labels_sampled: np.ndarray):
    """Ghép thành tập docs chung và lưu mapping cho từng cặp.

    E1_sampled: (M, D)
    E2_sampled: (M, D)
    labels_sampled: (M,)
    Trả về:
        docs: (2M, D)
        pair_info: list dict với keys: pair_index, doc1_id, doc2_id, label
    """
    M, D = E1_sampled.shape
    docs = np.vstack([E1_sampled, E2_sampled])  # (2M, D)

    pair_info = []
    for i in range(M):
        pair_info.append({
            'pair_index': int(i),
            'doc1_id': int(i),
            'doc2_id': int(i + M),
            'label': int(labels_sampled[i]),
        })
    return docs, pair_info

# =============================
# Hashing: SimHash & MinHash
# =============================

class HashEncoder:
    def fit(self, docs: np.ndarray):
        raise NotImplementedError

    def encode(self, docs: np.ndarray) -> np.ndarray:
        raise NotImplementedError


class SimHashEncoder:
    """
    SimHash trên embedding dạng vector thực, dùng random hyperplanes.
    - n_bits: số bit trong SimHash (thường 64 hoặc 128)
    - normalize: có chuẩn hoá vector đầu vào về norm=1 hay không
    - orthogonal_planes: có trực giao hoá các hyperplane hay không (tuỳ chọn)
    """
    def __init__(self, n_bits=128, normalize=True, orthogonal_planes=True, random_state=42):
        self.n_bits = int(n_bits)
        self.normalize = bool(normalize)
        self.orthogonal_planes = bool(orthogonal_planes)
        self.random_state = random_state
        self.random_planes = None

    def fit(self, X: np.ndarray):
        X = np.asarray(X, dtype=np.float32)
        n_features = X.shape[1]

        rng = np.random.default_rng(self.random_state)
        planes = rng.standard_normal(size=(self.n_bits, n_features), dtype=np.float32)

        if self.orthogonal_planes:
            # Trực giao hóa các hyperplane (QR decomposition)
            # Chỉ cần min(n_bits, n_features) vector trực giao đầu tiên
            Q, _ = np.linalg.qr(planes.T)  # shape: (n_features, n_features)
            Q = Q[:, : self.n_bits]        # (n_features, n_bits)
            self.random_planes = Q.T.astype(np.float32, copy=True)
        else:
            self.random_planes = planes

        return self

    def encode(self, X: np.ndarray) -> np.ndarray:
        """
        X: (n_samples, n_features)
        Trả về: bits uint8 0/1, shape (n_samples, n_bits)
        """
        if self.random_planes is None:
            raise RuntimeError("SimHashEncoder must be fitted before calling encode().")

        X = np.asarray(X, dtype=np.float32)
        if X.ndim == 1:
            X = X[None, :]

        if self.normalize:
            norms = np.linalg.norm(X, axis=1, keepdims=True) + 1e-9
            X = X / norms

        # (n_samples, n_features) @ (n_features, n_bits) -> (n_samples, n_bits)
        projections = X @ self.random_planes.T
        bits = (projections >= 0).astype(np.uint8)
        return bits

class MinHashEncoder:
    """
    MinHash trên embedding:
    - Chuyển mỗi vector thành một "tập" các chiều top-k có |value| lớn nhất.
    - Áp dụng n_hashes hàm băm tuyến tính trên chỉ số chiều để tạo chữ ký MinHash.
    """
    def __init__(self, n_hashes=128, topk_dims=20, random_state=42):
        self.n_hashes = int(n_hashes)
        self.topk_dims = int(topk_dims)
        self.random_state = random_state
        self.a = None
        self.b = None
        self.prime = None
        self.n_features = None

    def fit(self, X: np.ndarray):
        X = np.asarray(X, dtype=np.float32)
        self.n_features = X.shape[1]
        # Chọn một số nguyên tố lớn để mod (lớn hơn số chiều embedding thông thường)
        self.prime = 2_000_003
        rng = np.random.default_rng(self.random_state)
        self.a = rng.integers(1, self.prime, size=self.n_hashes, dtype=np.int64)
        self.b = rng.integers(0, self.prime, size=self.n_hashes, dtype=np.int64)
        return self

    def _vector_to_indices(self, x: np.ndarray) -> np.ndarray:
        """Lấy top-k chỉ số chiều theo |value|."""
        k = min(self.topk_dims, x.shape[0])
        if k == x.shape[0]:
            idx = np.arange(x.shape[0], dtype=np.int64)
        else:
            idx = np.argpartition(-np.abs(x), k - 1)[:k].astype(np.int64)
        return np.sort(idx)

    def encode(self, X: np.ndarray) -> np.ndarray:
        """
        X: (n_samples, n_features)
        Trả về: signatures MinHash shape (n_samples, n_hashes) kiểu int32.
        """
        X = np.asarray(X, dtype=np.float32)
        if X.ndim == 1:
            X = X[None, :]

        n_samples = X.shape[0]
        sigs = np.full(
            (n_samples, self.n_hashes),
            fill_value=np.iinfo(np.int32).max,
            dtype=np.int32,
        )

        for i in range(n_samples):
            idxs = self._vector_to_indices(X[i])
            if idxs.size == 0:
                continue
            # hashes: (n_hashes, len(idxs))
            hashes = (self.a[:, None] * idxs[None, :] + self.b[:, None]) % self.prime
            mins = hashes.min(axis=1).astype(np.int32)
            sigs[i] = mins

        return sigs

# =============================
# Index cho similarity search
# =============================

class BaseIndex:
    def build(self, docs: np.ndarray, signatures: np.ndarray = None):
        raise NotImplementedError

    def query(self, query_vec: np.ndarray, top_k: int = 10, encoder: HashEncoder = None):
        raise NotImplementedError


class ExactCosineIndex(BaseIndex):
    """Search exact theo cosine similarity (baseline)."""
    def build(self, docs: np.ndarray, signatures: np.ndarray = None):
        self.docs = docs
        norms = np.linalg.norm(docs, axis=1, keepdims=True) + 1e-9
        self.normalized = docs / norms

    def query(self, query_vec: np.ndarray, top_k: int = 10, encoder: HashEncoder = None):
        q = query_vec / (np.linalg.norm(query_vec) + 1e-9)
        scores = self.normalized @ q  # (N,)
        top_idx = np.argpartition(-scores, top_k)[:top_k]
        top_idx = top_idx[np.argsort(-scores[top_idx])]
        return [(int(i), float(scores[i])) for i in top_idx]


from collections import defaultdict, Counter

class SimHashLSHIndex(BaseIndex):
    """LSH cho SimHash với banding + re-rank cosine."""
    def __init__(
        self,
        n_bands: int = 8,
        min_match_bands: int = 1,
        use_cosine: bool = True,
        max_rerank: int = 200,
    ):
        self.n_bands = int(n_bands)
        self.min_match_bands = int(min_match_bands)
        self.use_cosine = bool(use_cosine)
        self.max_rerank = int(max_rerank)

        self.buckets = None      # List[dict]
        self.docs_norm = None    # (N, D)
        self.signatures = None   # (N, n_bits) 0/1
        self.band_width = None   # số bit mỗi band


    def build(self, docs: np.ndarray, signatures: np.ndarray):
        """
        docs: (N, D) embeddings gốc
        signatures: (N, n_bits) mảng uint8 (0 hoặc 1)
        """
        docs = np.asarray(docs, dtype=np.float32)
        signatures = np.asarray(signatures, dtype=np.uint8)

        # Lưu signatures để dùng lại (Hamming + query_from_bits)
        self.signatures = signatures

        # Chuẩn hoá docs một lần cho cosine
        norms = np.linalg.norm(docs, axis=1, keepdims=True) + 1e-9
        self.docs_norm = docs / norms

        N, n_bits = signatures.shape
        assert n_bits % self.n_bands == 0
        self.band_width = n_bits // self.n_bands

        # 1. Packbits toàn bộ ma trận: (N, n_bands, band_width) -> packbits -> (N, n_bands, bytes_width)
        reshaped_sigs = signatures.reshape(N, self.n_bands, self.band_width)
        packed_sigs = np.packbits(reshaped_sigs, axis=2)  # shape: (N, n_bands, bytes_width)

        # 2. Xây dựng Hash Tables
        # buckets là list gồm n_bands dictionary. Mỗi dict map: hash_key (bytes) -> list of doc_ids
        self.buckets = [defaultdict(list) for _ in range(self.n_bands)]

        for b in range(self.n_bands):
            # Flatten keys của band b về (N, bytes_width)
            keys_view = packed_sigs[:, b, :].reshape(N, -1)
            for doc_id, key_arr in enumerate(keys_view):
                key_bytes = key_arr.tobytes()
                self.buckets[b][key_bytes].append(doc_id)

    def _hamming_prefilter(self, q_bits: np.ndarray, candidate_ids: np.ndarray) -> np.ndarray:
        """
        Cắt bớt candidate bằng Hamming distance trên simhash bits.
        Giữ lại tối đa self.max_rerank doc gần nhất (Hamming nhỏ nhất).
        """
        if self.max_rerank is None or len(candidate_ids) <= self.max_rerank:
            return candidate_ids

        q_bits = np.asarray(q_bits, dtype=np.uint8)
        sig_sub = self.signatures[candidate_ids]  # (M, n_bits)
        # Hamming = số bit khác
        ham = np.count_nonzero(sig_sub != q_bits, axis=1)

        top = min(self.max_rerank, len(ham))
        top_idx = np.argpartition(ham, top - 1)[:top]
        return candidate_ids[top_idx]

    def query_from_bits(
        self,
        q_bits: np.ndarray,
        query_vec: np.ndarray = None,
        top_k: int = 10,
        use_hamming: bool = True,
        display: bool = False,
    ):
        """
        Query khi đã có sẵn simhash bits của query.
        - q_bits: (n_bits,) 0/1
        - query_vec: vector gốc (D,) nếu muốn cosine re-rank.
                     Nếu None và self.use_cosine=True thì sẽ fallback về Hamming.
        Trả về: list[(doc_id, score)]
        """
        q_bits = np.asarray(q_bits, dtype=np.uint8).ravel()

        # 1. Packbits q_bits thành các band
        n_bits = self.signatures.shape[1]
        assert len(q_bits) == n_bits
        band_width = self.band_width or (n_bits // self.n_bands)

        q_reshaped = q_bits.reshape(self.n_bands, band_width)
        q_packed = np.packbits(q_reshaped, axis=1)  # (n_bands, bytes_width)

        # 2. Collect candidates giống query() cũ
        candidates_list = []
        for b in range(self.n_bands):
            key = q_packed[b].tobytes()
            if key in self.buckets[b]:
                candidates_list.extend(self.buckets[b][key])

        if not candidates_list:
            return []

        # 3. Đếm số lần xuất hiện
        candidates_arr = np.array(candidates_list, dtype=int)
        unique_ids, counts = np.unique(candidates_arr, return_counts=True)

        # 4. Filter theo min_match_bands
        mask = counts >= self.min_match_bands
        final_candidates = unique_ids[mask]

        if len(final_candidates) == 0:
            # Fallback: lấy tất cả nếu không ai thỏa mãn ngưỡng
            final_candidates = unique_ids



        # 5. Prefilter bằng Hamming (giảm số lượng phải cosine)
        if use_hamming:
            final_candidates = self._hamming_prefilter(q_bits, final_candidates)

        # 6. Nếu không dùng cosine hoặc không có query_vec -> trả theo Hamming luôn
        if (not self.use_cosine) or (query_vec is None):
            sig_sub = self.signatures[final_candidates]
            ham = np.count_nonzero(sig_sub != q_bits, axis=1)
            # similarity ~ 1 - normalized_hamming
            sim = 1.0 - ham.astype(np.float32) / float(len(q_bits))

            # chọn top_k theo sim
            if len(sim) > top_k:
                top_idx = np.argpartition(-sim, top_k - 1)[:top_k]
                top_idx = top_idx[np.argsort(-sim[top_idx])]
            else:
                top_idx = np.argsort(-sim)

            results = []
            for idx in top_idx:
                doc_id = int(final_candidates[idx])
                score = float(sim[idx])
                results.append((doc_id, score))
            return results


        # === [BẠN THÊM CÂU LỆNH Ở ĐÂY] ===
        if display:
            print(f"Số lượng candidates cần rerank: {len(final_candidates)}")

        # =================================


        # 7. Dùng cosine re-rank (giống phần 6 cũ nhưng với final_candidates đã cắt)
        cand_vectors = self.docs_norm[final_candidates]  # (M, D)

        q_norm = query_vec / (np.linalg.norm(query_vec) + 1e-9)
        scores = cand_vectors @ q_norm

        if len(scores) > top_k:
            top_idx = np.argpartition(-scores, top_k - 1)[:top_k]
            top_idx = top_idx[np.argsort(-scores[top_idx])]
        else:
            top_idx = np.argsort(-scores)

        results = []
        for idx in top_idx:
            doc_id = int(final_candidates[idx])
            score = float(scores[idx])
            results.append((doc_id, score))

        return results


    def query(
        self,
        query_vec: np.ndarray,
        top_k: int = 10,
        encoder=None,
        use_hamming: bool = True,
    ):
        """
        Giữ API cũ: nhận query_vec + encoder.
        Nội bộ: encode sang q_bits rồi gọi query_from_bits().
        """
        if encoder is None:
            raise ValueError("encoder must be provided to encode query_vec to simhash bits.")

        q_bits = encoder.encode(query_vec[None, :])[0]  # (n_bits,)
        return self.query_from_bits(
            q_bits=q_bits,
            query_vec=query_vec,
            top_k=top_k,
            use_hamming=use_hamming,
        )

class MinHashLSHIndex(BaseIndex):
    """LSH cho MinHash signatures với banding (buckets theo từng band)."""
    def __init__(self, n_bands: int = 16, min_match_bands: int = 1):
        self.n_bands = int(n_bands)
        self.min_match_bands = int(min_match_bands)
        self.buckets = None
        self.signatures = None
        self.band_width = None

    def build(self, docs: np.ndarray, signatures: np.ndarray):
        """
        docs: (N, D) - không dùng trực tiếp, chỉ để giữ API cho thống nhất.
        signatures: (N, n_hashes) - MinHash signatures kiểu int32.
        """
        signatures = np.asarray(signatures, dtype=np.int32)
        self.signatures = signatures
        N, n_hashes = signatures.shape
        assert n_hashes % self.n_bands == 0, "n_hashes phải chia hết cho n_bands"
        self.band_width = n_hashes // self.n_bands

        # buckets[b]: dict[band_key(tuple)] -> list[doc_id]
        self.buckets = [defaultdict(list) for _ in range(self.n_bands)]
        for doc_id in range(N):
            sig = signatures[doc_id]
            for b in range(self.n_bands):
                start = b * self.band_width
                end = start + self.band_width
                band = tuple(sig[start:end].tolist())
                self.buckets[b][band].append(doc_id)

    def _collect_candidates(self, q_sig: np.ndarray) -> np.ndarray:
        candidates = []
        for b in range(self.n_bands):
            start = b * self.band_width
            end = start + self.band_width
            key = tuple(q_sig[start:end].tolist())
            if key in self.buckets[b]:
                candidates.extend(self.buckets[b][key])

        if not candidates:
            return np.array([], dtype=int)

        cand_arr = np.array(candidates, dtype=int)
        unique_ids, counts = np.unique(cand_arr, return_counts=True)
        mask = counts >= self.min_match_bands
        final = unique_ids[mask]
        if final.size == 0:
            final = unique_ids
        return final

    def query_from_signature(self, q_sig: np.ndarray, top_k: int = 10):
        """Query trực tiếp từ chữ ký MinHash q_sig."""
        q_sig = np.asarray(q_sig, dtype=np.int32).ravel()
        if self.signatures is None:
            raise RuntimeError("Index chưa được build với signatures.")
        assert q_sig.shape[0] == self.signatures.shape[1]

        final_candidates = self._collect_candidates(q_sig)
        if final_candidates.size == 0:
            return []

        sig_sub = self.signatures[final_candidates]
        # Ước lượng Jaccard ~ tỉ lệ số dòng trùng nhau trong chữ ký
        sim = (sig_sub == q_sig[None, :]).mean(axis=1).astype(np.float32)

        if len(sim) > top_k:
            top_idx = np.argpartition(-sim, top_k - 1)[:top_k]
            top_idx = top_idx[np.argsort(-sim[top_idx])]
        else:
            top_idx = np.argsort(-sim)

        results = []
        for idx in top_idx:
            doc_id = int(final_candidates[idx])
            score = float(sim[idx])
            results.append((doc_id, score))
        return results

    def query(self, query_vec: np.ndarray, top_k: int = 10, encoder: HashEncoder = None):
        if encoder is None:
            raise ValueError("encoder must be provided to encode query_vec to MinHash signature.")
        q_sig = encoder.encode(query_vec[None, :])[0]
        return self.query_from_signature(q_sig, top_k=top_k)

# =============================
# Evaluation với is_duplicate
# =============================

def evaluate_recall_at_k(index: BaseIndex,
                         docs: np.ndarray,
                         pair_info: list,
                         k: int,
                         encoder: HashEncoder = None,
                         signatures: np.ndarray = None,
                         use_hamming: bool = True):
    """
    Đánh giá Recall@K dựa trên nhãn is_duplicate.
    Hỗ trợ 3 trường hợp:
      - ExactCosineIndex (không dùng signatures) -> index.query(...)
      - SimHashLSHIndex  (có method query_from_bits) -> dùng trực tiếp bits đã precompute
      - MinHashLSHIndex  (có method query_from_signature) -> dùng trực tiếp chữ ký MinHash
    """
    hits = 0
    total = 0
    count = 0

    use_simhash_bits = (
        signatures is not None
        and hasattr(index, "query_from_bits")
    )
    use_minhash_sigs = (
        signatures is not None
        and hasattr(index, "query_from_signature")
    )

    for p in pair_info:
        if p["label"] != 1:
            continue

        total += 1
        doc1_id = p["doc1_id"]
        doc2_id = p["doc2_id"]

        query_vec = docs[doc1_id]

        # ===== LẤY HÀNG XÓM =====
        if use_simhash_bits:
            # Dùng SimHashLSHIndex.query_from_bits -> KHÔNG encode lại
            q_bits = signatures[doc1_id]
            if count < 5:
                neighbors = index.query_from_bits(
                    q_bits=q_bits,
                    query_vec=query_vec,
                    top_k=k + 1,
                    use_hamming=use_hamming,
                    display=True,
                )
                count += 1
            else:
                neighbors = index.query_from_bits(
                    q_bits=q_bits,
                    query_vec=query_vec,
                    top_k=k + 1,
                    use_hamming=use_hamming,
                )

        elif use_minhash_sigs:
            # Dùng MinHashLSHIndex.query_from_signature -> không cần query_vec
            q_sig = signatures[doc1_id]
            neighbors = index.query_from_signature(
                q_sig,
                top_k=k + 1,
            )

        else:
            # Fallback: dùng API cũ (cosine hoặc index khác)
            neighbors = index.query(
                query_vec,
                top_k=k + 1,
                encoder=encoder,
            )

        # neighbors: list[(doc_id, score)]
        # Bỏ self và cắt còn tối đa k phần tử
        neighbors = [(doc_id, score) for doc_id, score in neighbors
                     if doc_id != doc1_id][:k]

        retrieved_ids = [doc_id for doc_id, _ in neighbors]

        if doc2_id in retrieved_ids:
            hits += 1

    recall_at_k = hits / total if total > 0 else 0.0
    return recall_at_k, hits, total

# =============================
# Hàm chạy toàn bộ thí nghiệm
# =============================

def run_experiment(cfg: ExperimentConfig):
    print('Embedding file:', cfg.embedding_file)
    print('Sample pairs:', cfg.sample_pairs)
    print('Hash method:', cfg.hash_method)
    print('-----------------------------')

    # 1) Load & sample
    E1, E2, labels = load_embeddings_npz(cfg.embedding_file)
    print('Tổng số cặp ban đầu:', E1.shape[0])
    E1_s, E2_s, labels_s = sample_pairs(E1, E2, labels, cfg.sample_pairs)
    docs, pair_info = build_corpus(E1_s, E2_s, labels_s)
    print('Số docs trong corpus:', docs.shape[0])
    print('Số cặp sau khi sample:', len(pair_info))

    # 2) Build index
    encoder = None
    signatures = None

    if cfg.hash_method == 'none':
        encoder = None
        index = ExactCosineIndex()
        index.build(docs)

    elif cfg.hash_method == 'simhash':
        encoder = SimHashEncoder(
            cfg.n_bits,
            normalize=True,
            orthogonal_planes=True,
            random_state=42,
        )
        encoder.fit(docs)
        signatures = encoder.encode(docs)

        index = SimHashLSHIndex(
            cfg.n_bands,
            min_match_bands=5,
            use_cosine=True,
            max_rerank=500,
        )
        index.build(docs, signatures)

    elif cfg.hash_method == 'minhash':
        encoder = MinHashEncoder(
            n_hashes=cfg.n_hashes,
            topk_dims=cfg.topk_dims,
            random_state=42,
        )
        encoder.fit(docs)
        signatures = encoder.encode(docs)

        index = MinHashLSHIndex(
            n_bands=cfg.n_bands,
            min_match_bands=2,
        )
        index.build(docs, signatures)

    else:
        raise ValueError(f'Unknown hash_method: {cfg.hash_method}')

    # 3) Evaluate Recall@K
    for k in [1, 5, 10]:
        recall, hits, total = evaluate_recall_at_k(
            index=index,
            docs=docs,
            pair_info=pair_info,
            k=k,
            encoder=encoder,        # để nếu sau này dùng index khác vẫn xài chung hàm
            signatures=signatures,  # truyền chữ ký hash (SimHash/MinHash) nếu có
            use_hamming=True,       # chỉ dùng cho SimHash; MinHash sẽ bỏ qua
        )
        print(f'Recall@{k}: {recall:.4f}  (hits={hits} / total={total})')

cfg = ExperimentConfig(
    embedding_file=EMBEDDING_FILE,
    sample_pairs=5000,
    hash_method='simhash',   # 'none' | 'simhash' | 'minhash'
    n_bits=256,
    n_hashes=128,
    topk_dims=20,
    n_bands=32,
    top_k=10,
)

print('Cấu hình thí nghiệm:', cfg)
run_experiment(cfg)

cfg = ExperimentConfig(
    embedding_file=EMBEDDING_FILE,
    sample_pairs=5000,
    hash_method='minhash',   # 'none' | 'simhash' | 'minhash'
    n_bits=256,              # dùng cho SimHash, giữ nguyên cũng được
    n_hashes=256,            # số hash function cho MinHash
    topk_dims=256,            # số chiều top-k tạo "set"
    n_bands=64,              # 128 / 32 = 4 hash / band
    top_k=10,
)

print('Cấu hình thí nghiệm:', cfg)
run_experiment(cfg)