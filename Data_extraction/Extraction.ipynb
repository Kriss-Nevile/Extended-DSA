{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c3fa66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\DS\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Admin\\miniconda3\\envs\\DS\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "3e4f5278",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bf7554e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 88124 entries, 0 to 88123\n",
      "Data columns (total 3 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   text1         88124 non-null  object \n",
      " 1   text2         21624 non-null  object \n",
      " 2   is_duplicate  21624 non-null  float64\n",
      "dtypes: float64(1), object(2)\n",
      "memory usage: 2.0+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "file_path = '../Data/merged_dataset.csv'\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"File loaded successfully.\")\n",
    "    # print(df.head())\n",
    "else:\n",
    "    print(f\"File not found: {file_path}\")\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28418954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "88124"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2ff7dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    if pd.isnull(row.text2) and not pd.isnull(row.is_duplicate):\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12aa2916",
   "metadata": {},
   "source": [
    "### Dataset modifications\n",
    "Use 256 as the token limit, split all rows inside the datset into chunks less than 256 tokens (sentence-based)\n",
    "\n",
    "we can use the same dataset for every model if it falls within the token limit\n",
    "\n",
    "Max length:\n",
    "- Sent-T5: 256\n",
    "- InstructorEmbedding: 512\n",
    "- e5-base-v2: 512\n",
    "- bge-base-en-v1.5: 512\n",
    "\n",
    "\n",
    "Instructor Embedding does matter when spacing is irregular, especially for tokens like `., !,..etc`, but as long as we only split based on empty spaces we should be fine\n",
    "\n",
    "Since we can only modify `text1` and not `text2` without losing ground truth, we need to make sure all samples that have ground truth be less than 256 tokens\n",
    "\n",
    "Violations:\n",
    "- With all-miniLM-L6-V2: we have one violations at index 71032\n",
    "    - Remove index 71032\n",
    "- With Instructor Embeddings (this have the max length of 512 tokens, so it can easily work with the text we currently for all-miniLM-L6-V2):\n",
    "    - No violations\n",
    "- With e5-base-v2 (This model is less sensitive so different `., !` spacing won't affect it):\n",
    "    - No violations\n",
    "- with bge-base-en-v1.5 (Same goes for this model -- not as sensitive)\n",
    "    - No violations\n",
    "\n",
    "Next step create a seperate dataset for each model (✅):\n",
    "- Sent-T5 ✅\n",
    "- InstructorEmbedding ✅\n",
    "- e5-base-v2 ✅\n",
    "- bge-base-en-v1.5 ✅\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1a1e6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21624"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ground_truth_df = df[(df['text2'].notnull()) & (df['is_duplicate'].notnull())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6a7d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\DS\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Admin\\.cache\\huggingface\\hub\\models--BAAI--bge-base-en-v1.5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('BAAI/bge-base-en-v1.5')\n",
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd6303a",
   "metadata": {},
   "source": [
    "### New dataframe creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "4e2298f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_string(s, tokenizer, verbose=False) -> list:\n",
    "    \"\"\" Splits long text into smaller chunks based on sentence boundaries\"\"\"\n",
    "    import nltk\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    sentences = sent_tokenize(s)\n",
    "    \n",
    "    cap = tokenizer.model_max_length\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_tokens = 0\n",
    "\n",
    "    if verbose:\n",
    "        total = 0\n",
    "        print(f\"Splitting string of length {len(tokenizer.tokenize(s))} into chunks with max {cap} tokens.\")\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        tokenized = tokenizer.tokenize(sentence)\n",
    "        if verbose:\n",
    "            total += len(tokenized)\n",
    "            print(f\"Sentence length: {len(tokenized)}, Sequence: {sentence} Total so far: {total}\")\n",
    "        if current_tokens + len(tokenized) > cap:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "            current_tokens = len(tokenized)\n",
    "        else:\n",
    "            current_chunk += \" \" + sentence if current_chunk else sentence\n",
    "            current_tokens += len(tokenized)\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def reformat_dataframe(df: pd.DataFrame, tokenizer, verbose=False) -> pd.DataFrame:\n",
    "    \"\"\" Reformats the dataframe by splitting long texts into smaller chunks\"\"\"\n",
    "\n",
    "    result_df = pd.DataFrame(columns=['text1', 'text2', 'is_duplicate'])\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if not pd.isnull(row.text2): \n",
    "            \"\"\"Appends the full row\"\"\"\n",
    "            result_df = pd.concat([result_df, pd.DataFrame([row])], ignore_index=True)\n",
    "            continue\n",
    "        text = row['text1']\n",
    "        tokenized = tokenizer.tokenize(text)\n",
    "        if len(tokenized) > tokenizer.model_max_length:\n",
    "            if verbose:\n",
    "                print(f\"Index {index} text length {len(tokenized)} exceeds max length {tokenizer.model_max_length}. Splitting...\")\n",
    "            # split_string should return a list of text chunks instead\n",
    "            chunks = split_string(text, tokenizer, verbose=verbose)\n",
    "            if verbose:\n",
    "                print(f\"Split into {len(chunks)} chunks.\")\n",
    "            for chunk in chunks:\n",
    "                new_row = {'text1': chunk, 'text2': None, 'is_duplicate': None}\n",
    "                result_df = pd.concat([result_df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        else:\n",
    "            result_df = pd.concat([result_df, pd.DataFrame([row])], ignore_index=True)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "d871db8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = [\n",
    "    'BAAI/bge-base-en-v1.5',\n",
    "    'sentence-transformers/all-MiniLM-L6-v2',\n",
    "    'hkunlp/instructor-base',\n",
    "    'intfloat/e5-base-v2'\n",
    "]\n",
    "\n",
    "# resulting_dfs = {}\n",
    "\n",
    "# for model_name in model_names:\n",
    "\n",
    "#     reference_df = df.copy()\n",
    "\n",
    "#     if model_name == 'sentence-transformers/all-MiniLM-L6-v2':\n",
    "#         # remove the 71032 index row which causes issues with this model\n",
    "#         reference_df = reference_df.drop(index=71032)\n",
    "#         reference_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "#     print(f\"Processing for model: {model_name}\")\n",
    "#     model = SentenceTransformer(model_name)\n",
    "#     tokenizer = model.tokenizer\n",
    "#     reformatted_df = reformat_dataframe(reference_df, tokenizer, verbose=False)\n",
    "\n",
    "#     resulting_dfs[model_name] = reformatted_df\n",
    "\n",
    "#     output_file = f'../Data/{model_name.split(\"/\")[-1]}_dataset.xlsx'\n",
    "    \n",
    "#     with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
    "#         reformatted_df.to_excel(writer, index=False, sheet_name='Sheet1')\n",
    "\n",
    "#     print(f\"Reformatted data saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e015776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded bge-base-en-v1.5_dataset.xlsx with 93630 rows.\n",
      "In file bge-base-en-v1.5_dataset.xlsx, count of non-null text2: 21624, count of non-null isduplicate: 21624\n",
      "Loaded all-MiniLM-L6-v2_dataset.xlsx with 101942 rows.\n",
      "In file all-MiniLM-L6-v2_dataset.xlsx, count of non-null text2: 21623, count of non-null isduplicate: 21623\n",
      "Loaded instructor-base_dataset.xlsx with 94621 rows.\n",
      "In file instructor-base_dataset.xlsx, count of non-null text2: 21624, count of non-null isduplicate: 21624\n",
      "Loaded e5-base-v2_dataset.xlsx with 93630 rows.\n",
      "In file e5-base-v2_dataset.xlsx, count of non-null text2: 21624, count of non-null isduplicate: 21624\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "file_names = [\n",
    "    'bge-base-en-v1.5_dataset.xlsx',\n",
    "    'all-MiniLM-L6-v2_dataset.xlsx',\n",
    "    'instructor-base_dataset.xlsx',\n",
    "    'e5-base-v2_dataset.xlsx'\n",
    "]\n",
    "\n",
    "for index, file_name in enumerate(file_names):\n",
    "    file_path = os.path.join('../Data/', file_name)\n",
    "    if os.path.exists(file_path):\n",
    "        df = pd.read_excel(file_path)\n",
    "        print(f\"Loaded {file_name} with {len(df)} rows.\")\n",
    "    else:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "    \n",
    "\n",
    "    count_text2 = 0\n",
    "    count_null = 0\n",
    "    # model = SentenceTransformer(model_names[index])\n",
    "    # tokenizer = model.tokenizer\n",
    "\n",
    "    # dropped = False\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        if pd.notnull(row.text2):\n",
    "            count_text2 += 1\n",
    "        if pd.notnull(row.is_duplicate):\n",
    "            count_null += 1\n",
    "    \n",
    "    print(f\"In file {file_name}, count of non-null text2: {count_text2}, count of non-null isduplicate: {count_null}\")\n",
    "\n",
    "    # if dropped:\n",
    "    #     print(f\"Dropped rows exceeding token limit in {file_name}. Updating file...\")\n",
    "    #     df.reset_index(drop=True, inplace=True)\n",
    "    #     #update the excel file\n",
    "    #     with pd.ExcelWriter(file_path + '-version2.xlsx', engine='xlsxwriter') as writer:\n",
    "    #         df.to_excel(writer, index=False, sheet_name='Sheet1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e35140",
   "metadata": {},
   "source": [
    "### Apply a text - sentence-based split using the tokenizer of the corresponding model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a19700",
   "metadata": {},
   "source": [
    "### All-minilm-L6-V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e0a15da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\"\"\"This model is suitable for general-purpose sentence embeddings \n",
    "and it can extend to paragraphs\"\"\"\n",
    "\n",
    "tokenizer = model.tokenizer\n",
    "max_length = tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc43a955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 88127/88127 [22:34<00:00, 65.07it/s]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Stores the embeddings in a new column dataframe, that has columns 'embeddings' and 'no' \n",
    "is the row number in the original dataframe\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"Verify the final embeddings df have the same number of rows as the original df\"\"\"\n",
    "from tqdm import tqdm\n",
    "\n",
    "target_df = pd.DataFrame(columns=['embeddings', 'no'])\n",
    "\n",
    "embeddings_list = []\n",
    "no_list = []\n",
    "\n",
    "\n",
    "for row in tqdm(df.itertuples(), desc=\"Generating embeddings\", total=len(df)):\n",
    "    text1_embedding = model.encode(row.text1)\n",
    "    embeddings_list.append(text1_embedding)\n",
    "    no_list.append(row.Index)\n",
    "\n",
    "    if not pd.isnull(row.text2) and row.text2.strip() != \"\":\n",
    "        text2_embedding = model.encode(row.text2)\n",
    "        embeddings_list.append(text2_embedding)\n",
    "        no_list.append(row.Index)\n",
    "\n",
    "target_df = pd.DataFrame({'embeddings': embeddings_list, 'no': no_list})\n",
    "\n",
    "\"\"\"We can save using either .pkl or .parquet, pkl is easier to use in pandas, parquet is more efficient in storage\"\"\"\n",
    "output_path = '../Data/embeddings.pkl'\n",
    "target_df.to_pickle(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdaacbcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using alternative initialization due to error: INSTRUCTOR._load_sbert_model() got an unexpected keyword argument 'token'\n",
      "Fallback: Using sentence-transformers directly\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating Instructor embeddings: 100%|██████████| 88127/88127 [2:53:47<00:00,  8.45it/s]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instructor embeddings saved to ../Data/instructor_embeddings.pkl\n",
      "Shape of instructor embeddings dataframe: (109753, 2)\n",
      "Shape of original dataframe: (88127, 3)\n"
     ]
    }
   ],
   "source": [
    "# Install InstructorEmbedding if not already installed\n",
    "from InstructorEmbedding import INSTRUCTOR\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load the instructor model with compatibility fix\n",
    "try:\n",
    "    instructor_model = INSTRUCTOR('hkunlp/instructor-large')\n",
    "except TypeError as e:\n",
    "    # Alternative approach if there are compatibility issues\n",
    "    print(f\"Using alternative initialization due to error: {e}\")\n",
    "    try:\n",
    "        instructor_model = INSTRUCTOR('hkunlp/instructor-large', cache_folder=None)\n",
    "    except:\n",
    "        print(\"Fallback: Using sentence-transformers directly\")\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        instructor_model = SentenceTransformer('hkunlp/instructor-large')\n",
    "\n",
    "# Create a new dataframe for instructor embeddings\n",
    "instructor_df = pd.DataFrame(columns=['embeddings', 'no'])\n",
    "\n",
    "instructor_embeddings_list = []\n",
    "instructor_no_list = []\n",
    "\n",
    "# Define instruction for the embedding task\n",
    "instruction = \"Represent the text for similarity comparison:\"\n",
    "\n",
    "for row in tqdm(df.itertuples(), desc=\"Generating Instructor embeddings\", total=len(df)):\n",
    "    # Encode text1 with instruction\n",
    "    text1_instructor_embedding = instructor_model.encode([[instruction, row.text1]])\n",
    "    instructor_embeddings_list.append(text1_instructor_embedding[0])  # Get the first (and only) embedding\n",
    "    instructor_no_list.append(row.Index)\n",
    "    \n",
    "    # Encode text2 if it exists and is not empty\n",
    "    if not pd.isnull(row.text2) and row.text2.strip() != \"\":\n",
    "        text2_instructor_embedding = instructor_model.encode([[instruction, row.text2]])\n",
    "        instructor_embeddings_list.append(text2_instructor_embedding[0])\n",
    "        instructor_no_list.append(row.Index)\n",
    "\n",
    "# Create the instructor embeddings dataframe\n",
    "instructor_df = pd.DataFrame({'embeddings': instructor_embeddings_list, 'no': instructor_no_list})\n",
    "\n",
    "# Save the instructor embeddings\n",
    "instructor_output_path = '../Data/instructor_embeddings.pkl'\n",
    "instructor_df.to_pickle(instructor_output_path)\n",
    "\n",
    "print(f\"Instructor embeddings saved to {instructor_output_path}\")\n",
    "print(f\"Shape of instructor embeddings dataframe: {instructor_df.shape}\")\n",
    "print(f\"Shape of original dataframe: {df.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
