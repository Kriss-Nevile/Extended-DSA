{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c81574d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\miniconda3\\envs\\DS\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Admin\\miniconda3\\envs\\DS\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "names = ['all-miniLM-L6-v2', 'bge-base-en-v1.5', 'e5-base-v2', 'instructor-base']\n",
    "\n",
    "def load_model(model_name: str) -> SentenceTransformer:\n",
    "    \"\"\"Load a SentenceTransformer model by name.\"\"\"\n",
    "    if model_name == 'all-miniLM-L6-v2':\n",
    "        model_name = 'sentence-transformers/all-miniLM-L6-v2'\n",
    "    elif model_name == 'bge-base-en-v1.5':\n",
    "        model_name = 'BAAI/bge-base-en-v1.5'\n",
    "    elif model_name == 'e5-base-v2':\n",
    "        model_name = 'intfloat/e5-base-v2'\n",
    "    elif model_name == 'instructor-base':\n",
    "        model_name = 'hkunlp/instructor-base'\n",
    "\n",
    "    return SentenceTransformer(model_name, device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d809b87f",
   "metadata": {},
   "source": [
    "## Build FAISS Index\n",
    "Create a FAISS index for efficient similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f491964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(model, text_list):\n",
    "    embeddings = model.encode(text_list, convert_to_numpy=True, show_progress_bar=True, batch_size=16)\n",
    "    return embeddings\n",
    "\n",
    "def get_faiss_index(name: str, load_model_ = False):\n",
    "    \"\"\"Returns the FAISS index, texts, and embedding dimension for the specified model name.\"\"\"\n",
    "    \n",
    "    save_path = f'Data/FAISSINDEX/{name}.bin'\n",
    "    model = None\n",
    "    \n",
    "    if os.path.exists(save_path):\n",
    "        index = faiss.read_index(save_path)\n",
    "        print(f\"FAISS index loaded from {save_path} with {index.ntotal} vectors.\")\n",
    "        dataset = pd.read_excel(f'Data/{name}_dataset.xlsx')\n",
    "        text1 = dataset.text1\n",
    "        text2 = dataset.text2[dataset.text2.notnull()]\n",
    "        texts = pd.concat([text1, text2]).tolist()\n",
    "        dimension = index.d\n",
    "        if load_model:\n",
    "            model = load_model(name)\n",
    "        return index, texts, dimension, model\n",
    "\n",
    "    dataset = pd.read_excel(f'Data/{name}_dataset.xlsx')\n",
    "    text1 = dataset.text1\n",
    "    text2 = dataset.text2[dataset.text2.notnull()]\n",
    "    texts = pd.concat([text1, text2]).tolist() \n",
    "\n",
    "    model = load_model(name)\n",
    "    embeddings = create_embeddings(model, texts)\n",
    "    dimension = embeddings.shape[1]\n",
    "    \n",
    "    index = faiss.IndexFlatIP(dimension)\n",
    "    faiss.normalize_L2(embeddings)\n",
    "    index.add(embeddings)\n",
    "\n",
    "    print(f\"FAISS index created for model: {name} with {index.ntotal} vectors.\")\n",
    "\n",
    "    if not os.path.exists('Data/FAISSINDEX'):\n",
    "        os.makedirs('Data/FAISSINDEX')\n",
    "\n",
    "    faiss.write_index(index, save_path)\n",
    "    print(f\"FAISS index saved to {save_path}\")\n",
    "\n",
    "    return index, texts, dimension, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0832a035",
   "metadata": {},
   "source": [
    "## Test Similarity Search\n",
    "Search for similar texts using a query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d75e67ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to search for similar texts\n",
    "def search_similar_texts(model, index, texts, query, k=5):\n",
    "    \"\"\"\n",
    "    Search for k most similar texts to the query\n",
    "    \n",
    "    Args:\n",
    "        query: Query text string\n",
    "        k: Number of similar results to return\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with similar texts and their distances\n",
    "    \"\"\"\n",
    "    # Encode the query\n",
    "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
    "    \n",
    "    # Search the index\n",
    "    distances, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results = pd.DataFrame({\n",
    "        'rank': range(1, k+1),\n",
    "        'text': [texts[idx] for idx in indices[0]],\n",
    "        'distance': distances[0],\n",
    "        'index': indices[0]\n",
    "    })\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ec5d66",
   "metadata": {},
   "source": [
    "## Save FAISS Index (Optional)\n",
    "Save the index and embeddings for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3176d026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index loaded from Data/FAISSINDEX/all-miniLM-L6-v2.bin with 123565 vectors.\n",
      "FAISS index loaded from Data/FAISSINDEX/bge-base-en-v1.5.bin with 115254 vectors.\n",
      "FAISS index loaded from Data/FAISSINDEX/bge-base-en-v1.5.bin with 115254 vectors.\n",
      "FAISS index loaded from Data/FAISSINDEX/e5-base-v2.bin with 115254 vectors.\n",
      "FAISS index loaded from Data/FAISSINDEX/e5-base-v2.bin with 115254 vectors.\n",
      "FAISS index loaded from Data/FAISSINDEX/instructor-base.bin with 116245 vectors.\n",
      "FAISS index loaded from Data/FAISSINDEX/instructor-base.bin with 116245 vectors.\n"
     ]
    }
   ],
   "source": [
    "indexes = {}\n",
    "for name in names:\n",
    "    index, texts, dimension, model = get_faiss_index(name, load_model_=False)\n",
    "    indexes[name] = {\n",
    "        'index': index,\n",
    "        'texts': texts,\n",
    "        'dimension': dimension,\n",
    "        'model': model\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34c7270",
   "metadata": {},
   "source": [
    "### Key benchmark is on the ground truth samples\n",
    "\n",
    "Evaluate with difference tolerance: 1, 3, 5.\n",
    "We have only 7085 rows of true duplicate samples, so we have to test on those for accuracy.\n",
    "\n",
    "Clustering based evaluation can be performed on the negative samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ebd6c156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Which are the best books to read on Digital Marketing?',\n",
       " 'What are some good books on Digital marketing?')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mapping for the values\n",
    "tolerances = [1, 3, 5, 10]\n",
    "\n",
    "name = 'instructor-base'\n",
    "index, texts, dimension, model = indexes[name].values()\n",
    "\n",
    "dataset = pd.read_excel(f'Data/{name}_dataset.xlsx')\n",
    "\n",
    "start1 = dataset[dataset.text2.notnull()].index[0]\n",
    "start2 = len(dataset)\n",
    "indices = dataset[dataset.is_duplicate.apply(lambda x: True if x > 0 else False)].index\n",
    "\n",
    "\n",
    "texts[start1 + 6], texts[start2 + 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "821b9981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating tolerance 1: 100%|██████████| 7085/7085 [06:00<00:00, 19.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tolerance_1': {'accuracy': 0.638955539872971, 'correct': 4527, 'total': 7085}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating tolerance 3: 100%|██████████| 7085/7085 [05:49<00:00, 20.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tolerance_1': {'accuracy': 0.638955539872971, 'correct': 4527, 'total': 7085},\n",
      " 'tolerance_3': {'accuracy': 0.8056457304163727,\n",
      "                 'correct': 5708,\n",
      "                 'total': 7085}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating tolerance 5: 100%|██████████| 7085/7085 [05:36<00:00, 21.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tolerance_1': {'accuracy': 0.638955539872971, 'correct': 4527, 'total': 7085},\n",
      " 'tolerance_3': {'accuracy': 0.8056457304163727,\n",
      "                 'correct': 5708,\n",
      "                 'total': 7085},\n",
      " 'tolerance_5': {'accuracy': 0.8585744530698659,\n",
      "                 'correct': 6083,\n",
      "                 'total': 7085}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating tolerance 10: 100%|██████████| 7085/7085 [05:07<00:00, 23.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tolerance_1': {'accuracy': 0.638955539872971, 'correct': 4527, 'total': 7085},\n",
      " 'tolerance_10': {'accuracy': 0.9110797459421313,\n",
      "                  'correct': 6455,\n",
      "                  'total': 7085},\n",
      " 'tolerance_3': {'accuracy': 0.8056457304163727,\n",
      "                 'correct': 5708,\n",
      "                 'total': 7085},\n",
      " 'tolerance_5': {'accuracy': 0.8585744530698659,\n",
      "                 'correct': 6083,\n",
      "                 'total': 7085}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Only accuracy is considered, since precision and recall is not applicable if there are no classes\"\"\"\n",
    "from tqdm import tqdm\n",
    "\n",
    "save_path = f\"Evaluation/FAISS/{name}.json\"\n",
    "\n",
    "results = {}\n",
    "\n",
    "for tolerance in tolerances:\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    for idx in tqdm(indices, desc=f\"Evaluating tolerance {tolerance}\"):\n",
    "        query = texts[idx]\n",
    "        result = search_similar_texts(model, index, texts, query, k=tolerance + 1)\n",
    "        if idx - start1 + start2 in result['index'].values:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    accuracy = correct / total\n",
    "    results[f'tolerance_{tolerance}'] = {\n",
    "        'total': total,\n",
    "        'correct': correct,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ab3eb767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(save_path, 'w') as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc85815a",
   "metadata": {},
   "source": [
    "### Perform text deduplication\n",
    "- Option:\n",
    "    - Using some consine similarity threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "49cfa5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def deduplicate_texts_with_mapping(model, index, texts, threshold=0.9):\n",
    "    \"\"\"\n",
    "    Deduplicate texts using FAISS index and cosine similarity threshold.\n",
    "    Returns:\n",
    "        unique_texts: list of unique texts\n",
    "        mapping: dict {unique_index: [represented_indices]}\n",
    "    \"\"\"\n",
    "    from tqdm import tqdm\n",
    "    unique_indices = []\n",
    "    seen = set()\n",
    "    mapping = {}\n",
    "    for i, text in tqdm(enumerate(texts), total=len(texts), desc=\"Deduplicating\"):\n",
    "        if i in seen:\n",
    "            continue\n",
    "        # Search for top 10 similar texts (including itself)\n",
    "        query_embedding = model.encode([text], convert_to_numpy=True)\n",
    "        distances, indices_ = index.search(query_embedding, 500)\n",
    "        group = [i]\n",
    "        for dist, idx in zip(distances[0], indices_[0]):\n",
    "            if idx != i:\n",
    "                if dist > threshold:\n",
    "                    seen.add(idx)\n",
    "                    group.append(idx)\n",
    "                else: break\n",
    "        else:\n",
    "            print(\"All index checked there might be even more duplicates\")\n",
    "        unique_indices.append(i)\n",
    "        mapping[i] = group\n",
    "    unique_texts = [texts[i] for i in unique_indices]\n",
    "    return unique_texts, mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b94226d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deduplicating: 100%|██████████| 116245/116245 [49:52<00:00, 38.84it/s] \n"
     ]
    }
   ],
   "source": [
    "index, texts, dimension, model = indexes[name].values()\n",
    "unique_texts, mapping = deduplicate_texts_with_mapping(model, index, texts, threshold=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fbf1ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unique_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43munique_texts\u001b[49m[\u001b[32m0\u001b[39m], mapping[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mNameError\u001b[39m: name 'unique_texts' is not defined"
     ]
    }
   ],
   "source": [
    "unique_texts[0], mapping[0]\n",
    "\"dasdas\" (0)\n",
    "[0, 1234, 5678, 213, 1]  # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "82b6489d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "86",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m86\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mKeyError\u001b[39m: 86"
     ]
    }
   ],
   "source": [
    "mapping[86]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ecca4553",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "86",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m dedup_pd = pd.DataFrame()\n\u001b[32m      5\u001b[39m dedup_pd[\u001b[33m'\u001b[39m\u001b[33munique_text\u001b[39m\u001b[33m'\u001b[39m] = unique_texts\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m dedup_pd[\u001b[33m'\u001b[39m\u001b[33mrepresented_indices\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mdedup_pd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\DS\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6491\u001b[39m, in \u001b[36mIndex.map\u001b[39m\u001b[34m(self, mapper, na_action)\u001b[39m\n\u001b[32m   6455\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   6456\u001b[39m \u001b[33;03mMap values using an input mapping or function.\u001b[39;00m\n\u001b[32m   6457\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   6487\u001b[39m \u001b[33;03mIndex(['A', 'B', 'C'], dtype='object')\u001b[39;00m\n\u001b[32m   6488\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   6489\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mindexes\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmulti\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MultiIndex\n\u001b[32m-> \u001b[39m\u001b[32m6491\u001b[39m new_values = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6493\u001b[39m \u001b[38;5;66;03m# we can return a MultiIndex\u001b[39;00m\n\u001b[32m   6494\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m new_values.size \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_values[\u001b[32m0\u001b[39m], \u001b[38;5;28mtuple\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\DS\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[39m, in \u001b[36mIndexOpsMixin._map_values\u001b[39m\u001b[34m(self, mapper, na_action, convert)\u001b[39m\n\u001b[32m    918\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m arr.map(mapper, na_action=na_action)\n\u001b[32m--> \u001b[39m\u001b[32m921\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m=\u001b[49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Admin\\miniconda3\\envs\\DS\\Lib\\site-packages\\pandas\\core\\algorithms.py:1743\u001b[39m, in \u001b[36mmap_array\u001b[39m\u001b[34m(arr, mapper, na_action, convert)\u001b[39m\n\u001b[32m   1741\u001b[39m values = arr.astype(\u001b[38;5;28mobject\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1743\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1745\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.map_infer_mask(\n\u001b[32m   1746\u001b[39m         values, mapper, mask=isna(values).view(np.uint8), convert=convert\n\u001b[32m   1747\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mlib.pyx:2972\u001b[39m, in \u001b[36mpandas._libs.lib.map_infer\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[70]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m      4\u001b[39m dedup_pd = pd.DataFrame()\n\u001b[32m      5\u001b[39m dedup_pd[\u001b[33m'\u001b[39m\u001b[33munique_text\u001b[39m\u001b[33m'\u001b[39m] = unique_texts\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m dedup_pd[\u001b[33m'\u001b[39m\u001b[33mrepresented_indices\u001b[39m\u001b[33m'\u001b[39m] = dedup_pd.index.map(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mmapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m)\n",
      "\u001b[31mKeyError\u001b[39m: 86"
     ]
    }
   ],
   "source": [
    "path = \"Evaluation/FAISS/Deduplication dataset\"\n",
    "os.path.exists(path)\n",
    "\n",
    "dedup_pd = pd.DataFrame()\n",
    "dedup_pd['unique_text'] = unique_texts\n",
    "dedup_pd['represented_indices'] = dedup_pd.index.map(lambda x: mapping[x])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
